{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train all text Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  line-based iterator that reads the file one line at a time instead of reading everything in memory at once\n",
    "import os\n",
    "class MySentences(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename,encoding='utf-8'):\n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a memory-friendly iterator\n",
    "sentences = MySentences('C:/Thesis/Data/save/Master_Data/lemmatized_data/data_lemmatized_latest.txt') # a memory-friendly iterator\n",
    "\n",
    "# NOTE:\n",
    "# sentences is now kept as a memory-friendly iterator and the contents of the txt file are now NEVER fully loaded into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MySentences"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Thesis/Data/save/Master_Data/lemmatized_data/data_lemmatized_latest.txt\", \"r\",encoding=\"utf-8\") as outfile: \n",
    "     filter_hrm = outfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(filter_hrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the word frequency of list sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordFreq(corpus):\n",
    "    result = {}\n",
    "    for data in corpus:\n",
    "        for word in data:\n",
    "            if word in result:\n",
    "                result[word] += 1 #adding result in the dictionary\n",
    "            else:\n",
    "                result[word] = 1\n",
    "\n",
    "    return result #returning full dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1 = getWordFreq(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether given key already exists in a dictionary. \n",
    "def checkKey(dict, key): \n",
    "      \n",
    "    if key in dict.keys(): \n",
    "        print(\"Present, \", end =\" \") \n",
    "        print(\"value =\", dict[key]) \n",
    "    else: \n",
    "        print(\"Not present\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present,  value = 11130\n"
     ]
    }
   ],
   "source": [
    "key = 'gement'\n",
    "checkKey(fdist1, key) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word list that needs to be excluded\n",
    "import pandas as pd\n",
    "df = pd.read_excel('C:/Users/ICTO-EB/Google Drive/myThesis/evaluation_testdataSet/exclude_list.xlsx', sep=r'\\s*,\\s*',header=0, encoding='ascii')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = df['word_list'].values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['above-mentioned', 'ago', 'alabama', 'amsterdam', 'angeles',\n",
       "       'anova', 'appendix', 'arizona', 'associatedwith', 'association.',\n",
       "       'augustacademy', 'aviv', 'behav', 'being', 'benefi', 'cacy',\n",
       "       'canton', 'carlo', 'carolina', 'chi-square', 'cial', 'cient',\n",
       "       'coeffi', 'connecticut', '-degree', 'doi', 'east', 'elsevier.',\n",
       "       'email', 'e-mail', 'emnce', 'emncy', 'experi', 'experienceater',\n",
       "       'extrarole', 'f', 'fax', 'for-invested', 'ghent', 'greenb',\n",
       "       'harvard', 'havnerally', 'high-', 'him-', 'hoc', 'htened', 'htens',\n",
       "       'identi', 'identifi', 'identiwcation', 'illinois', 'impor',\n",
       "       'indcol', 'individual-', 'individuallevel', 'ingroup', 'inrole',\n",
       "       'insofar', 'interrole', 'intra-', '-item', 'juneacademy',\n",
       "       'knippenb', 'louisiana', 'low-', 'ment', 'ments', 'meta',\n",
       "       'metaanalyses', 'metaanalysis', 'metaanalytic', 'michigan',\n",
       "       'microlevel', 'minnesota', 'monte', '-month', 'mwork',\n",
       "       'notwithstanding', 'nan', 'ohio', 'ought', 'over-', '-pa',\n",
       "       'pennsylvania', '-point', 'priori', 'purdue', 'pygmalion',\n",
       "       'queensland', 'quid', 'quo', 'resource.', 'reusidelines',\n",
       "       'rotterdam', 'self-', 'short-', 'somewhat', 'sub', 'syn', 'taiwan',\n",
       "       'thereof', 'thesoups', 'thesps', 'thework', 'tim', 'tom',\n",
       "       'towhich', 'universiteit', 'university', 'us', 'vandenbe',\n",
       "       'vidual', '-vis', 'vis-a', '-week', 'wisconsin', 'within-',\n",
       "       '-year', 'yes', 'zealand', 'zurich', 'ZYX', 'aap', 'aaps', 'abc',\n",
       "       'abei', 'ac', 'acc', 'acknowl', 'acknowld', 'acknowlent',\n",
       "       'acknowlents', 'acknowlment', 'acknowlments', 'acknowlng',\n",
       "       'acknowls', 'aco', 'ad', 'adm', 'aet', 'aji', 'amj', 'anglo',\n",
       "       'ann', 'anti-sh', 'anuscript', 'aoc', 'apgo', 'apple', 'arab',\n",
       "       'areater', 'aremore', 'aring', 'arneralizable', 'arnerally',\n",
       "       'arod', 'arven', 'asa', 'asd', 'ass', 'aston', 'avect', 'avected',\n",
       "       'bambr', 'batna', 'bene', 'benewts', 'bneralizable', 'bneralized',\n",
       "       'bnerally', 'bos', 'ca', 'cantly', 'cata', 'cation', 'cbca', 'cc',\n",
       "       'ce', 'cfa', 'cfi', 'challd', 'challng', 'chall-oriented',\n",
       "       'challs', 'chrps', 'cip', 'cji', 'cmv', 'cob', 'coc', 'cod',\n",
       "       'colle', 'confi', 'confl', 'conv', 'convnce', 'convng', 'convnt',\n",
       "       'cor', 'cpa', 'cpbs', 'cpm', 'cpt', 'crl', 'cs', 'cse', 'csir',\n",
       "       'csp', 'csr', 'cvc', 'cvf', 'cwb', 'cwb-i', 'cwb-o', 'cwbs', 'da',\n",
       "       'dc', 'ddr', 'defi', 'dem', 'dems', 'dence', 'dep', 'der', 'dif',\n",
       "       'diff', 'dis', 'disge', 'disged', 'div', 'diver', 'divnce',\n",
       "       'divng', 'divnt', 'dot', 'double-d', 'ds', 'eap', 'eaps', 'ect',\n",
       "       'ective', 'ee', 'egb', 'ehmp', 'ehmps', 'eisenbr', 'eld', 'eli',\n",
       "       'em', 'emd', 'emng', 'emnt', 'emt', 'ence', 'ences', 'ene',\n",
       "       'entic', 'enzed', 'eor', 'epm', 'eq-i', 'er', 'erent', 'eri', 'es',\n",
       "       'esearch', 'esm', 'esvps', 'eva', 'evect', 'evective', 'evects',\n",
       "       'exibility', 'fdr', 'fds', 'ffi', 'ffm', 'fig', 'fiw', 'flms',\n",
       "       'fmla', 'formance', 'frcc', 'frp', 'fsb', 'fshc', 'fssb', 'ftf',\n",
       "       'ftp', 'ful', 'fwa', 'fwas', 'fwc', 'fwe', 'gaphic', 'gaphical',\n",
       "       'gaphically', 'ge', 'ge√¢', 'ged', 'gement', 'gma', 'gocb', 'gpa',\n",
       "       'gse', 'guanxi', 'haveater', 'hcns', 'hcws', 'high-lmx', 'hihrs',\n",
       "       'hiwp', 'hiwps', 'hop', 'hphr', 'hphrps', 'hpwp', 'hpwps', 'hpws',\n",
       "       'hpwss', 'hqrs', 'hrps', 'hsms', 'hten', 'ibts', 'icb', 'icc',\n",
       "       'i-deal', 'i-deals', 'iem', 'ien', 'ier', 'ifd', 'ifi', 'iii',\n",
       "       'ijv', 'ijvs', 'ili', 'ily', 'indi', 'infl', 'ing', 'intoup',\n",
       "       'introject', 'inverted-u', 'i-o', 'ior', 'iot', 'i-p', 'ipa',\n",
       "       'ipo', 'ipt', 'irb', 'irp', 'ish', 'issn', 'ith', 'ity', 'iwb',\n",
       "       'jcm', 'jdc', 'jd-r', 'jugaad', 'jv', 'knowl-based',\n",
       "       'knowl-intensive', 'ksaos', 'ksas', 'kw', 'lgb', 'lgo', 'lhy',\n",
       "       'li', 'listserv', 'lli', 'llx', 'lmg-p', 'lmx', 'lmx-mdm', 'loc',\n",
       "       'los', 'lsc', 'lsh', 'ltd', 'lto', 'maintenancals', 'mance',\n",
       "       'mbea', 'mbi', 'mbo', 'mgpm', 'mhr', 'mid-s', 'min', 'mnc', 'mncs',\n",
       "       'mne', 'mnes', 'mng', 'morobal', 'mp', 'mr', 'msa', 'msf', 'mspr',\n",
       "       'msprs', 'mst', 'mt', 'mtl', 'mtm', 'multi', 'multiplal', 'mws',\n",
       "       'nal', 'nancial', 'ncsr', 'nder', 'ndered', 'ndering', 'nders',\n",
       "       'nding', 'ndings', 'ned', 'neering', 'neers', 'ness', 'nizational',\n",
       "       'non', 'npd', 'npm', 'npos', 'obhdp', 'ocm', 'oeem', 'oes', 'oid',\n",
       "       'ojp', 'ols', 'onal', 'orga', 'organi', 'organiza', 'osc', 'osi',\n",
       "       'osphi', 'otp', 'outw', 'overwt', 'owe', 'pap', 'par', 'pas',\n",
       "       'pbc', 'pbi', 'p-bjw', 'pbp', 'pcf', 'pco', 'pcsr', 'p-e', 'peo',\n",
       "       'peos', 'perfor', 'performancins', 'pfp', 'pgat', 'phd', 'phone',\n",
       "       'phr', 'pi', 'p-j', 'ple', 'pmc', 'pns', 'po', 'p-o', 'pob', 'poc',\n",
       "       'poi', 'post-mr', 'power-distance', 'powerholders', 'ppgo', 'ppms',\n",
       "       'practicoup', 'pre', 'pre-', 'pre-mr', 'promes', 'prot', 'prote√¢',\n",
       "       'ps', 'psc', 'psf', 'psychol', 'pti', 'pwb', 'pwd', 'rbse', 'rbv',\n",
       "       'rch', 'rct', 'rdt', 'rela', 'rev', 'rhq', 'riasec', 'rip', 'rjps',\n",
       "       'rlmx', 'rlw', 'rms', 'rmsea', 'roa', 'rogelb', 'rpo', 'rsi',\n",
       "       'rst', 'rticle', 'rutgers', 'rwg', 's', 'sat', 'satis', 'sbp',\n",
       "       'scd', 'scm', 'sct', 'sdo', 'sdt', 'sem', 'ser', 'sies', 'signi',\n",
       "       'signifi', 'sion', 'sip', 'smt', 'soc', 'soe', 'soes', 'specifi',\n",
       "       'speciwc', 'sphr', 'spj', 'strh', 'strhen', 'strhened',\n",
       "       'strhening', 'strhens', 'strhs', 'strhs-based', 'strota', 'svo',\n",
       "       'swb', 'synes', 'synstic', 'tavistock', 'taw', 'tda', 'tel', 'tfl',\n",
       "       'th', 'thal', 'thals', 'thass', 'theat', 'thesals', 'thme',\n",
       "       'thnder', 'thneral', 'thneralizability', 'thneralization',\n",
       "       'thnerally', 'thneration', 'thnetic', 'thobal', 'thobe', 'thod',\n",
       "       'thosals', 'thoup', 'thoups', 'thowing', 'thowth', 'thp', 'thrman',\n",
       "       'thven', 'thvernance', 'thvernment', 'tic', 'tio', 'tion',\n",
       "       'tional', 'tions', 'tionship', 'tip', 'tiv', 'tive', 'tively',\n",
       "       'tlc', 'tm', 'tmm', 'tmms', 'tmt', 'tmts', 'tmx', 'tpb', 'tra',\n",
       "       'tsm', 'ture', 'udo', 'uence', 'uenced', 'uences', 'und',\n",
       "       'undaduates', 'unding', 'uthor', 'van', 'var', 'vcs', 'versa',\n",
       "       'vice', 'w', 'wbi', 'wernerally', 'werven', 'wfc', 'wfe', 'wfi',\n",
       "       'whi', 'wif', 'wlbs', 'wlc', 'wle', 'wndings', 'wrst', 'wted',\n",
       "       'wting', 'wts', 'XYZ', 'zational', 'gements'], dtype='<U16')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter exclude word list from the dict\n",
    "for key in df_list:\n",
    "    if key in fdist1:\n",
    "        del fdist1[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fdist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dict to df\n",
    "import pandas as pd\n",
    "df_fdist = pd.DataFrame.from_dict(fdist1, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_fdist = df_fdist.sort_values(by=[0], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the threshold to remove the certain section of vocabulary\n",
    "theta = 0.96    \n",
    "df_threshold = df_fdist[df_fdist[0].cumsum()/df_fdist[0].sum() < theta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minValue = df_threshold[0].min()\n",
    "print(minValue)\n",
    "print(len(df_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to excel\n",
    "df_threshold.to_excel(\"C:/Thesis/Data/save/Master_Data/Model/latest/word2vec/word_occurance_list.xlsx\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some memory clean-up\n",
    "del fdist1\n",
    "del df_fdist\n",
    "del df_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trained with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "epochs=200\n",
    "#sentences = list_sent\n",
    "model_bestpara = Word2Vec(\n",
    "        sentences, # our dataset\n",
    "        size=100, # this is the length of the vector to numerically represent the \"meaning\" of words\n",
    "        window=15, # this is the number of neighboring words to consider when assigning \"meaning\" to a word\n",
    "        min_count=minValue, # minimum number of occurrences\n",
    "        alpha = 0.005,\n",
    "        iter =  epochs) # this is how fast the model adapts its representation of the \"meaning\" of a word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_bestpara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(model_bestpara.wv.most_similar(positive = ['engagement'], topn=10), columns = ['word', 'similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model with 96% percentile \n",
    "model_bestpara.save('C:/Thesis/Data/save/Master_Data/Model/latest/word2vec/word2vec_model1_96_percentile.model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model1\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_bestpara = Word2Vec.load(\"C:/Thesis/Data/save/Master_Data/Model/latest/word2vec/word2vec_model1_96_percentile.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_bestpara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model2\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_2 = Word2Vec.load(\"C:/Thesis/Data/save/Master_Data/Model/latest/word2vec/word2vec_model2_96_percentile.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(model_2.wv.most_similar(positive = ['engagement'], topn=10), columns = ['word', 'similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model3\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_3 = Word2Vec.load(\"C:/Thesis/Data/save/Master_Data/Model/latest/word2vec/word2vec_model3_96_percentile.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(model_3.wv.most_similar(positive = ['engagement'], topn=10), columns = ['word', 'similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(model_bestpara.wv.most_similar(positive = ['satisfaction'], topn=10), columns = ['word', 'similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model with 95% percentile\n",
    "model_bestpara.save('C:/Thesis/Data/save/Master_Data/Model/latest/word2vec/word2vec_model1.model') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
