{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from nltk import SnowballStemmer\n",
    "from gensim.models import Word2Vec\n",
    "#import langdetect\n",
    "import tika\n",
    "import time\n",
    "from tika import parser\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read line by line, run lemmatizationand write result line by line to new file\n",
    "datafile = \"C:/Thesis/Data/save/Master_Data/pre_processed_data/data_preprocessed_txt.txt\" # cant upload it in Github because of its size\n",
    "tgtfile  = \"C:/Thesis/Data/save/Master_Data/lemmatized_data/data_lemmatized_latest.txt\"\n",
    "\n",
    "cutoff_word_length = 2\n",
    "cutoff_sent_length = 5\n",
    "\n",
    "with open(tgtfile, 'a', encoding='utf8') as outfile:\n",
    "    for line in open(datafile, 'r', encoding='utf8'):\n",
    "        temp = re.sub('\\n', '', line)\n",
    "        words = temp.split(' ')\n",
    "        lemmas = [lmtzr.lemmatize(w, get_wordnet_pos(w)) for w in words if len(w) > cutoff_word_length]\n",
    "        if (len(lemmas) > cutoff_sent_length):\n",
    "            lemmas = ' '.join(lemmas) + '\\n'\n",
    "            outfile.write(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
